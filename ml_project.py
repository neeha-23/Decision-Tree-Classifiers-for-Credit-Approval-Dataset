# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9URM0reYvd8QVHluanMiBhTFqRGN-EX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from math import log2

# Load dataset
def load_data(file_path):
    data = pd.read_csv(file_path, header=None, na_values='?')
    return data

# Load and preprocess data
training_data = load_data('/content/training.data')
test_data = load_data('/content/test.data')

X_train, y_train = split_features_labels(training_data)
X_test, y_test = split_features_labels(test_data)

X_train = handle_missing_values(X_train, X_train)
X_test = handle_missing_values(X_test, X_train)

# Train trees with cross-validation
c45_tree = cross_validate(X_train, y_train, build_c45_tree)
cart_tree = cross_validate(X_train, y_train, build_cart_tree)

# Evaluate on test set
c45_predictions = [predict(c45_tree, row) for _, row in X_test.iterrows()]
cart_predictions = [predict(cart_tree, row) for _, row in X_test.iterrows()]

# Compute metrics
c45_metrics = compute_metrics(y_test, c45_predictions)
cart_metrics = compute_metrics(y_test, cart_predictions)

# Split attributes and labels
def split_features_labels(data):
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]
    return X, y

# Handle missing values using median for numerical and sorted median for categorical
def handle_missing_values(X, X_train):
    print("Before handling missing values:")
    print(X.head())
    for col in X.columns:
        if X[col].dtype == 'object':
            sorted_values = sorted(X_train[col].dropna().unique())
            median_value = sorted_values[len(sorted_values) // 2] if sorted_values else None
        else:
            median_value = X_train[col].median()
        X[col] = X[col].fillna(median_value)
        print(f"Column: {col}, Filled Missing Value: {median_value}")  # Debugging line
    print("After handling missing values:")
    print(X.head())
    return X

# Entropy calculation
def entropy(y):
    counts = Counter(y)
    total = len(y)
    return -sum((count/total) * log2(count/total) for count in counts.values() if count > 0)

# Information gain calculation
def information_gain(X, y, feature):
    """Calculates the information gain for a given feature."""
    total_entropy = entropy(y)
    weighted_entropy = 0
    for value in X[feature].unique():
        subset_y = y[X[feature] == value]
        weighted_entropy += (len(subset_y) / len(y)) * entropy(subset_y)
    return total_entropy - weighted_entropy

# Information gain ratio for C4.5
def information_gain_ratio(X, y, feature):
    gain = information_gain(X, y, feature) # Now, this call works!
    split_info = entropy(X[feature])
    return gain / split_info if split_info != 0 else gain
# Gini index for CART
def gini_index(y):
    counts = Counter(y)
    total = len(y)
    return 1 - sum((count/total) ** 2 for count in counts.values())

# Implement C4.5 (basic structure)
def build_c45_tree(X, y):
    if len(set(y)) == 1:
        return y.iloc[0]
    best_feature = max(X.columns, key=lambda feature: information_gain_ratio(X, y, feature))
    tree = {best_feature: {}}
    for value in X[best_feature].unique():
        subtree_X = X[X[best_feature] == value].drop(columns=[best_feature])
        subtree_y = y[X[best_feature] == value]
        tree[best_feature][value] = build_c45_tree(subtree_X, subtree_y)
    return tree

# Implement CART (basic structure using Gini index)
def build_cart_tree(X, y):
    if len(set(y)) == 1:
        return y.iloc[0]
    best_feature = min(X.columns, key=lambda feature: gini_index(y[X[feature].notnull()]))
    tree = {best_feature: {}}
    for value in X[best_feature].unique():
        subtree_X = X[X[best_feature] == value].drop(columns=[best_feature])
        subtree_y = y[X[best_feature] == value]
        tree[best_feature][value] = build_cart_tree(subtree_X, subtree_y)
    return tree

# Function to visualize the tree
def print_tree(tree, depth=0):
    if not isinstance(tree, dict):
        print("  " * depth, "->", tree)
        return
    for key, value in tree.items():
        print("  " * depth, key)
        for subkey, subtree in value.items():
            print("  " * (depth + 1), subkey)
            print_tree(subtree, depth + 2)

# Manual 10-fold cross-validation
def cross_validate(X, y, build_tree_func):
    fold_size = len(X) // 10
    best_tree, best_f1 = None, 0
    for i in range(10):
        start, end = i * fold_size, (i + 1) * fold_size
        X_val, y_val = X.iloc[start:end], y.iloc[start:end]
        X_train = pd.concat([X.iloc[:start], X.iloc[end:]])
        y_train = pd.concat([y.iloc[:start], y.iloc[end:]])
        tree = build_tree_func(X_train, y_train)
        predictions = [predict(tree, row) for _, row in X_val.iterrows()]
        accuracy, precision, recall, f1 = compute_metrics(y_val, predictions)
        if f1 > best_f1:
            best_tree, best_f1 = tree, f1
    return best_tree

# Prediction function
def predict(tree, row):
    if not isinstance(tree, dict):
        return tree
    feature = next(iter(tree))
    value = row[feature]
    return predict(tree[feature].get(value, list(tree[feature].values())[0]), row)

print("C4.5 Decision Tree:")
print_tree(c45_tree)
print("\nCART Decision Tree:")
print_tree(cart_tree)

# Compute evaluation metrics
def compute_metrics(y_true, y_pred):
    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == '+' and yp == '+')
    tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == '-' and yp == '-')
    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == '-' and yp == '+')
    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == '+' and yp == '-')
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    return accuracy, precision, recall, f1# Plot results
labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
c45_values = list(c45_metrics)
cart_values = list(cart_metrics)

# Output results
print("C4.5 Decision Tree:")
print("Accuracy:", c45_metrics[0])
print("Precision:", c45_metrics[1])
print("Recall:", c45_metrics[2])
print("F1 Score:", c45_metrics[3])

print("\nCART Decision Tree:")
print("Accuracy:", cart_metrics[0])
print("Precision:", cart_metrics[1])
print("Recall:", cart_metrics[2])
print("F1 Score:", cart_metrics[3])

print("\nComparison: C4.5 performs better" if c45_metrics[3] > cart_metrics[3] else "\nComparison: CART performs better")

plt.figure(figsize=(8, 5))
plt.bar(labels, c45_values, alpha=0.9, label='C4.5')
plt.bar(labels, cart_values, alpha=0.6, label='CART')
plt.legend()
plt.title('Performance Comparison')
plt.show()